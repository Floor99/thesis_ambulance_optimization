defaults:
  - override hydra/sweeper: optuna
  - override hydra/sweeper/sampler: tpe
  - override hydra/launcher: joblib

hydra:
  sweeper:
    sampler:
      seed: 123
    direction: maximize
    study_name: sphere
    storage: null
    n_trials: 20
    n_jobs: 4
    params:
      # decoder.num_heads: choice(2, 4, 8)
      reward_mod.penalty_per_step_value: choice(-1.0, -5.0, -10.0, -20.0)
      decoder.learning_rate: tag(log, interval(1e-4, 1e-1))
      reinforce.discount_factor: interval(0.90, 0.999)
      reinforce.entropy_coeff: tag(log, interval(1e-4, 0.1))
      reinforce.baseline_loss_coeff: tag(log, interval(0.01, 10.0))

decoder:
  num_heads: 8
  embedding_dim: 128
  learning_rate: 0.01
  dropout: 0.1

stat_enc:
  num_layers: 8
  num_heads: 8
  hidden_size: 128
  out_size: 128
  dropout: 0.2
  learning_rate: 0.001

dyn_enc:
  num_layers: 8
  num_heads: 8
  hidden_size: 128
  out_size: 128
  dropout: 0.2
  learning_rate: 0.001

baseline:
  hidden_size: 256
  dropout: 0.2
  learning_rate: 0.001

reinforce:
  discount_factor: 0.99
  entropy_coeff: 0.0
  baseline_loss_coeff: 1
  max_grad_norm: 0.5

training:
  batch_size: 8
  num_epochs: 1000
  max_steps: 80
  patience: 10
  early_stopping: True
  save_best_model: True
  log_interval: 10

reward_mod:
  revisit_penalty_value: -50.0
  penalty_per_step_value: -5.0
  goal_bonus_value: 1000.0
  dead_end_penalty_value: -1000.0
  higher_speed_bonus_value: 20.0
  closer_to_goal_bonus_value: 2
  aggregated_step_penalty_value: -10.0
  no_signal_intersection_penalty_value: -10.0

