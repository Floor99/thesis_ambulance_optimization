{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05461f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "# set working directory to the directory where this notebook is located\n",
    "import os\n",
    "os.chdir(os.path.dirname(os.path.abspath(\"__file__\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feb07b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_nodes_from_timeseries(ts, n=10, seed=None):\n",
    "    \"\"\"\n",
    "    Get random nodes from a time series DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    ts (pd.DataFrame): Time series data with nodes as columns.\n",
    "    n (int): Number of random nodes to select.\n",
    "    seed (int, optional): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the selected random nodes.\n",
    "    \"\"\"\n",
    "    \n",
    "    nodes = pd.Series(ts[\"node_id\"].unique())  # Get unique node IDs from the DataFrame\n",
    "    if n > len(nodes):\n",
    "        raise ValueError(\"n cannot be greater than the number of unique nodes in the DataFrame.\")\n",
    "    if n <= 0:\n",
    "        raise ValueError(\"n must be a positive integer.\")\n",
    "    if not isinstance(ts, pd.DataFrame):\n",
    "        raise TypeError(\"ts must be a pandas DataFrame.\")\n",
    "    \n",
    "    selected_nodes = nodes.sample(n=n, replace=False, random_state=seed)  # Randomly select nodes\n",
    "    return selected_nodes.tolist()\n",
    "\n",
    "\n",
    "def set_smart_traffic_lights_for_nodes_in_timeseries(ts, nodes):\n",
    "    \n",
    "    \"\"\"\n",
    "    Set smart traffic lights for specified nodes in a time series DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    ts (pd.DataFrame): Time series data with nodes as columns.\n",
    "    nodes (list): List of node IDs to set smart traffic lights for.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Updated DataFrame with smart traffic lights set for specified nodes.\n",
    "    \"\"\"\n",
    "    if not isinstance(ts, pd.DataFrame):\n",
    "        raise TypeError(\"ts must be a pandas DataFrame.\")\n",
    "    \n",
    "    if not all(node in ts[\"node_id\"].unique() for node in nodes):\n",
    "        raise ValueError(\"All nodes must be present in the time series DataFrame.\")\n",
    "    \n",
    "    # Set smart traffic light and wait time for specified nodes, 0 for smart traffic light otherwise\n",
    "    ts[\"smart_traffic_light\"] = 0\n",
    "    ts.loc[ts[\"node_id\"].isin(nodes), \"smart_traffic_light\"] = 1\n",
    "    ts.loc[ts[\"node_id\"].isin(nodes), \"status\"] = 1\n",
    "    ts.loc[ts[\"node_id\"].isin(nodes), \"wait_time\"] = 0\n",
    "    return ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "464c2b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def get_data_obj_paths(data_objects_root):\n",
    "    data_obj_paths = []\n",
    "    for folder in os.listdir(data_objects_root):\n",
    "        if folder.startswith(\"network_\") and os.path.isdir(os.path.join(data_objects_root, folder)):\n",
    "            folder_paths = os.listdir(os.path.join(data_objects_root, folder))\n",
    "            folder_paths = [os.path.join(data_objects_root, folder, path) for path in folder_paths if path.endswith(\".pt\")]\n",
    "            data_obj_paths.extend(folder_paths)\n",
    "    return data_obj_paths\n",
    "\n",
    "\n",
    "def get_data_objects(data_obj_paths:list[str]):\n",
    "    def load_data_obj(path):\n",
    "        return torch.load(path, weights_only=False)\n",
    "\n",
    "    data_objects = []\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        for i, data_obj in enumerate(executor.map(load_data_obj, data_obj_paths)):\n",
    "            print(f\"Loading object {i+1}/{len(data_obj_paths)}\")\n",
    "            data_objects.append(data_obj)\n",
    "    return data_objects\n",
    "\n",
    "\n",
    "def add_smart_traffic_lights_to_data_object(data_object:Data, nodes):    \n",
    "    ts = data_object.timeseries\n",
    "    ts = set_smart_traffic_lights_for_nodes_in_timeseries(ts, nodes)\n",
    "    data_object.timeseries = ts\n",
    "    \n",
    "    # start_node \n",
    "    # end_node\n",
    "    # time_series\n",
    "    # x\n",
    "    # edge_index\n",
    "    # edge_attr\n",
    "    # G_sub\n",
    "    # G_pt\n",
    "    \n",
    "    new_features = torch.tensor(\n",
    "        ts.drop_duplicates(subset=\"node_id\")[\"smart_traffic_light\"].values,\n",
    "        dtype=torch.float32\n",
    "    ).unsqueeze(1)\n",
    "    new_features = torch.cat((data_object.x, new_features), dim=1)\n",
    "    \n",
    "    new_data_object = Data(\n",
    "        x=new_features,\n",
    "        edge_index=data_object.edge_index,\n",
    "        edge_attr=data_object.edge_attr,\n",
    "    )\n",
    "    new_data_object.start_node = data_object.start_node\n",
    "    new_data_object.end_node = data_object.end_node\n",
    "    new_data_object.time_series = data_object.timeseries\n",
    "    new_data_object.G_sub = data_object.G_sub\n",
    "    new_data_object.G_pt = data_object.G_pt\n",
    "   \n",
    "    return new_data_object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af66f3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading object 1/64\n",
      "Loading object 2/64\n",
      "Loading object 3/64\n",
      "Loading object 4/64\n",
      "Loading object 5/64\n",
      "Loading object 6/64\n",
      "Loading object 7/64\n",
      "Loading object 8/64\n",
      "Loading object 9/64\n",
      "Loading object 10/64\n",
      "Loading object 11/64\n",
      "Loading object 12/64\n",
      "Loading object 13/64\n",
      "Loading object 14/64\n",
      "Loading object 15/64\n",
      "Loading object 16/64\n",
      "Loading object 17/64\n",
      "Loading object 18/64\n",
      "Loading object 19/64\n",
      "Loading object 20/64\n",
      "Loading object 21/64\n",
      "Loading object 22/64\n",
      "Loading object 23/64\n",
      "Loading object 24/64\n",
      "Loading object 25/64\n",
      "Loading object 26/64\n",
      "Loading object 27/64\n",
      "Loading object 28/64\n",
      "Loading object 29/64\n",
      "Loading object 30/64\n",
      "Loading object 31/64\n",
      "Loading object 32/64\n",
      "Loading object 33/64\n",
      "Loading object 34/64\n",
      "Loading object 35/64\n",
      "Loading object 36/64\n",
      "Loading object 37/64\n",
      "Loading object 38/64\n",
      "Loading object 39/64\n",
      "Loading object 40/64\n",
      "Loading object 41/64\n",
      "Loading object 42/64\n",
      "Loading object 43/64\n",
      "Loading object 44/64\n",
      "Loading object 45/64\n",
      "Loading object 46/64\n",
      "Loading object 47/64\n",
      "Loading object 48/64\n",
      "Loading object 49/64\n",
      "Loading object 50/64\n",
      "Loading object 51/64\n",
      "Loading object 52/64\n",
      "Loading object 53/64\n",
      "Loading object 54/64\n",
      "Loading object 55/64\n",
      "Loading object 56/64\n",
      "Loading object 57/64\n",
      "Loading object 58/64\n",
      "Loading object 59/64\n",
      "Loading object 60/64\n",
      "Loading object 61/64\n",
      "Loading object 62/64\n",
      "Loading object 63/64\n",
      "Loading object 64/64\n",
      "Loading object 1/32\n",
      "Loading object 2/32\n",
      "Loading object 3/32\n",
      "Loading object 4/32\n",
      "Loading object 5/32\n",
      "Loading object 6/32\n",
      "Loading object 7/32\n",
      "Loading object 8/32\n",
      "Loading object 9/32\n",
      "Loading object 10/32\n",
      "Loading object 11/32\n",
      "Loading object 12/32\n",
      "Loading object 13/32\n",
      "Loading object 14/32\n",
      "Loading object 15/32\n",
      "Loading object 16/32\n",
      "Loading object 17/32\n",
      "Loading object 18/32\n",
      "Loading object 19/32\n",
      "Loading object 20/32\n",
      "Loading object 21/32\n",
      "Loading object 22/32\n",
      "Loading object 23/32\n",
      "Loading object 24/32\n",
      "Loading object 25/32\n",
      "Loading object 26/32\n",
      "Loading object 27/32\n",
      "Loading object 28/32\n",
      "Loading object 29/32\n",
      "Loading object 30/32\n",
      "Loading object 31/32\n",
      "Loading object 32/32\n",
      "Loading object 1/32\n",
      "Loading object 2/32\n",
      "Loading object 3/32\n",
      "Loading object 4/32\n",
      "Loading object 5/32\n",
      "Loading object 6/32\n",
      "Loading object 7/32\n",
      "Loading object 8/32\n",
      "Loading object 9/32\n",
      "Loading object 10/32\n",
      "Loading object 11/32\n",
      "Loading object 12/32\n",
      "Loading object 13/32\n",
      "Loading object 14/32\n",
      "Loading object 15/32\n",
      "Loading object 16/32\n",
      "Loading object 17/32\n",
      "Loading object 18/32\n",
      "Loading object 19/32\n",
      "Loading object 20/32\n",
      "Loading object 21/32\n",
      "Loading object 22/32\n",
      "Loading object 23/32\n",
      "Loading object 24/32\n",
      "Loading object 25/32\n",
      "Loading object 26/32\n",
      "Loading object 27/32\n",
      "Loading object 28/32\n",
      "Loading object 29/32\n",
      "Loading object 30/32\n",
      "Loading object 31/32\n",
      "Loading object 32/32\n"
     ]
    }
   ],
   "source": [
    "train_data_objects_paths = get_data_obj_paths(\"data/training_data_2/not_smart\")\n",
    "val_data_objects_paths = get_data_obj_paths(\"data/validation_data_2/not_smart\")\n",
    "test_data_objects_paths = get_data_obj_paths(\"data/test_data_2/not_smart\")\n",
    "\n",
    "\n",
    "train_data_objects = get_data_objects(train_data_objects_paths)\n",
    "val_data_objects = get_data_objects(val_data_objects_paths)\n",
    "test_data_objects = get_data_objects(test_data_objects_paths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59117311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/training_data_2/smart_lights/network_8/network_8_34_159.pt\n",
      "data/training_data_2/smart_lights/network_8/network_8_202_354.pt\n",
      "data/training_data_2/smart_lights/network_8/network_8_90_228.pt\n",
      "data/training_data_2/smart_lights/network_8/network_8_339_81.pt\n",
      "data/training_data_2/smart_lights/network_13/network_13_28_116.pt\n",
      "data/training_data_2/smart_lights/network_13/network_13_244_124.pt\n",
      "data/training_data_2/smart_lights/network_13/network_13_213_12.pt\n",
      "data/training_data_2/smart_lights/network_13/network_13_183_27.pt\n",
      "data/training_data_2/smart_lights/network_3/network_3_17_21.pt\n",
      "data/training_data_2/smart_lights/network_3/network_3_196_104.pt\n",
      "data/training_data_2/smart_lights/network_3/network_3_11_180.pt\n",
      "data/training_data_2/smart_lights/network_3/network_3_279_127.pt\n",
      "data/training_data_2/smart_lights/network_7/network_7_129_126.pt\n",
      "data/training_data_2/smart_lights/network_7/network_7_145_124.pt\n",
      "data/training_data_2/smart_lights/network_7/network_7_5_112.pt\n",
      "data/training_data_2/smart_lights/network_7/network_7_115_101.pt\n",
      "data/training_data_2/smart_lights/network_10/network_10_5_6.pt\n",
      "data/training_data_2/smart_lights/network_10/network_10_5_0.pt\n",
      "data/training_data_2/smart_lights/network_10/network_10_8_13.pt\n",
      "data/training_data_2/smart_lights/network_10/network_10_12_15.pt\n",
      "data/training_data_2/smart_lights/network_5/network_5_33_8.pt\n",
      "data/training_data_2/smart_lights/network_5/network_5_54_189.pt\n",
      "data/training_data_2/smart_lights/network_5/network_5_264_208.pt\n",
      "data/training_data_2/smart_lights/network_5/network_5_146_227.pt\n",
      "data/training_data_2/smart_lights/network_2/network_2_293_12.pt\n",
      "data/training_data_2/smart_lights/network_2/network_2_192_55.pt\n",
      "data/training_data_2/smart_lights/network_2/network_2_283_207.pt\n",
      "data/training_data_2/smart_lights/network_2/network_2_301_152.pt\n",
      "data/training_data_2/smart_lights/network_12/network_12_116_6.pt\n",
      "data/training_data_2/smart_lights/network_12/network_12_93_98.pt\n",
      "data/training_data_2/smart_lights/network_12/network_12_184_37.pt\n",
      "data/training_data_2/smart_lights/network_12/network_12_189_115.pt\n",
      "data/training_data_2/smart_lights/network_4/network_4_251_303.pt\n",
      "data/training_data_2/smart_lights/network_4/network_4_313_258.pt\n",
      "data/training_data_2/smart_lights/network_4/network_4_370_214.pt\n",
      "data/training_data_2/smart_lights/network_4/network_4_320_270.pt\n",
      "data/training_data_2/smart_lights/network_14/network_14_315_194.pt\n",
      "data/training_data_2/smart_lights/network_14/network_14_287_201.pt\n",
      "data/training_data_2/smart_lights/network_14/network_14_22_276.pt\n",
      "data/training_data_2/smart_lights/network_14/network_14_319_247.pt\n",
      "data/training_data_2/smart_lights/network_11/network_11_130_157.pt\n",
      "data/training_data_2/smart_lights/network_11/network_11_165_256.pt\n",
      "data/training_data_2/smart_lights/network_11/network_11_12_131.pt\n",
      "data/training_data_2/smart_lights/network_11/network_11_142_18.pt\n",
      "data/training_data_2/smart_lights/network_0/network_0_216_248.pt\n",
      "data/training_data_2/smart_lights/network_0/network_0_226_215.pt\n",
      "data/training_data_2/smart_lights/network_0/network_0_28_282.pt\n",
      "data/training_data_2/smart_lights/network_0/network_0_235_193.pt\n",
      "data/training_data_2/smart_lights/network_9/network_9_45_125.pt\n",
      "data/training_data_2/smart_lights/network_9/network_9_4_76.pt\n",
      "data/training_data_2/smart_lights/network_9/network_9_331_42.pt\n",
      "data/training_data_2/smart_lights/network_9/network_9_240_288.pt\n",
      "data/training_data_2/smart_lights/network_15/network_15_160_249.pt\n",
      "data/training_data_2/smart_lights/network_15/network_15_242_39.pt\n",
      "data/training_data_2/smart_lights/network_15/network_15_208_198.pt\n",
      "data/training_data_2/smart_lights/network_15/network_15_115_220.pt\n",
      "data/training_data_2/smart_lights/network_1/network_1_139_40.pt\n",
      "data/training_data_2/smart_lights/network_1/network_1_33_183.pt\n",
      "data/training_data_2/smart_lights/network_1/network_1_209_105.pt\n",
      "data/training_data_2/smart_lights/network_1/network_1_145_224.pt\n",
      "data/training_data_2/smart_lights/network_6/network_6_300_391.pt\n",
      "data/training_data_2/smart_lights/network_6/network_6_161_223.pt\n",
      "data/training_data_2/smart_lights/network_6/network_6_54_15.pt\n",
      "data/training_data_2/smart_lights/network_6/network_6_197_237.pt\n",
      "data/validation_data_2/smart_lights/network_3/network_3_43_293.pt\n",
      "data/validation_data_2/smart_lights/network_3/network_3_157_249.pt\n",
      "data/validation_data_2/smart_lights/network_3/network_3_219_61.pt\n",
      "data/validation_data_2/smart_lights/network_3/network_3_220_16.pt\n",
      "data/validation_data_2/smart_lights/network_7/network_7_174_169.pt\n",
      "data/validation_data_2/smart_lights/network_7/network_7_302_145.pt\n",
      "data/validation_data_2/smart_lights/network_7/network_7_97_59.pt\n",
      "data/validation_data_2/smart_lights/network_7/network_7_66_282.pt\n",
      "data/validation_data_2/smart_lights/network_5/network_5_25_274.pt\n",
      "data/validation_data_2/smart_lights/network_5/network_5_19_255.pt\n",
      "data/validation_data_2/smart_lights/network_5/network_5_47_308.pt\n",
      "data/validation_data_2/smart_lights/network_5/network_5_280_115.pt\n",
      "data/validation_data_2/smart_lights/network_2/network_2_97_184.pt\n",
      "data/validation_data_2/smart_lights/network_2/network_2_162_45.pt\n",
      "data/validation_data_2/smart_lights/network_2/network_2_164_35.pt\n",
      "data/validation_data_2/smart_lights/network_2/network_2_93_15.pt\n",
      "data/validation_data_2/smart_lights/network_4/network_4_180_254.pt\n",
      "data/validation_data_2/smart_lights/network_4/network_4_204_77.pt\n",
      "data/validation_data_2/smart_lights/network_4/network_4_240_237.pt\n",
      "data/validation_data_2/smart_lights/network_4/network_4_218_44.pt\n",
      "data/validation_data_2/smart_lights/network_0/network_0_205_103.pt\n",
      "data/validation_data_2/smart_lights/network_0/network_0_159_40.pt\n",
      "data/validation_data_2/smart_lights/network_0/network_0_139_77.pt\n",
      "data/validation_data_2/smart_lights/network_0/network_0_94_202.pt\n",
      "data/validation_data_2/smart_lights/network_1/network_1_80_4.pt\n",
      "data/validation_data_2/smart_lights/network_1/network_1_14_248.pt\n",
      "data/validation_data_2/smart_lights/network_1/network_1_27_295.pt\n",
      "data/validation_data_2/smart_lights/network_1/network_1_177_334.pt\n",
      "data/validation_data_2/smart_lights/network_6/network_6_216_372.pt\n",
      "data/validation_data_2/smart_lights/network_6/network_6_228_383.pt\n",
      "data/validation_data_2/smart_lights/network_6/network_6_214_176.pt\n",
      "data/validation_data_2/smart_lights/network_6/network_6_246_360.pt\n",
      "data/test_data_2/smart_lights/network_3/network_3_10_8.pt\n",
      "data/test_data_2/smart_lights/network_3/network_3_6_14.pt\n",
      "data/test_data_2/smart_lights/network_3/network_3_8_11.pt\n",
      "data/test_data_2/smart_lights/network_3/network_3_1_0.pt\n",
      "data/test_data_2/smart_lights/network_7/network_7_191_205.pt\n",
      "data/test_data_2/smart_lights/network_7/network_7_390_271.pt\n",
      "data/test_data_2/smart_lights/network_7/network_7_16_130.pt\n",
      "data/test_data_2/smart_lights/network_7/network_7_234_210.pt\n",
      "data/test_data_2/smart_lights/network_5/network_5_62_159.pt\n",
      "data/test_data_2/smart_lights/network_5/network_5_215_40.pt\n",
      "data/test_data_2/smart_lights/network_5/network_5_84_194.pt\n",
      "data/test_data_2/smart_lights/network_5/network_5_209_387.pt\n",
      "data/test_data_2/smart_lights/network_2/network_2_85_81.pt\n",
      "data/test_data_2/smart_lights/network_2/network_2_22_16.pt\n",
      "data/test_data_2/smart_lights/network_2/network_2_2_97.pt\n",
      "data/test_data_2/smart_lights/network_2/network_2_11_17.pt\n",
      "data/test_data_2/smart_lights/network_4/network_4_177_106.pt\n",
      "data/test_data_2/smart_lights/network_4/network_4_132_182.pt\n",
      "data/test_data_2/smart_lights/network_4/network_4_297_163.pt\n",
      "data/test_data_2/smart_lights/network_4/network_4_287_303.pt\n",
      "data/test_data_2/smart_lights/network_0/network_0_51_297.pt\n",
      "data/test_data_2/smart_lights/network_0/network_0_79_64.pt\n",
      "data/test_data_2/smart_lights/network_0/network_0_122_5.pt\n",
      "data/test_data_2/smart_lights/network_0/network_0_286_25.pt\n",
      "data/test_data_2/smart_lights/network_1/network_1_0_13.pt\n",
      "data/test_data_2/smart_lights/network_1/network_1_13_237.pt\n",
      "data/test_data_2/smart_lights/network_1/network_1_201_45.pt\n",
      "data/test_data_2/smart_lights/network_1/network_1_36_274.pt\n",
      "data/test_data_2/smart_lights/network_6/network_6_12_8.pt\n",
      "data/test_data_2/smart_lights/network_6/network_6_25_21.pt\n",
      "data/test_data_2/smart_lights/network_6/network_6_14_0.pt\n",
      "data/test_data_2/smart_lights/network_6/network_6_6_0.pt\n"
     ]
    }
   ],
   "source": [
    "for data_objects, file_path in [\n",
    "    (train_data_objects, train_data_objects_paths),\n",
    "    (val_data_objects, val_data_objects_paths),\n",
    "    (test_data_objects, test_data_objects_paths)\n",
    "]:\n",
    "    for i, data_object in enumerate(data_objects):\n",
    "        nodes = get_random_nodes_from_timeseries(data_object.timeseries, n=10, seed=42)\n",
    "        data_objects[i] = add_smart_traffic_lights_to_data_object(data_object, nodes)\n",
    "        folders = file_path[i].split(os.sep)\n",
    "        new_folder = \"smart_lights\"\n",
    "        new_dir = os.path.join(*folders[:-3], new_folder, *folders[3:])\n",
    "        print(new_dir)\n",
    "        os.makedirs(os.path.dirname(new_dir), exist_ok=True)\n",
    "        torch.save(data_object, new_dir)\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "637da770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_objects_paths"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
